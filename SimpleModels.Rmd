---
title: "Simple Models"
author: "Daniel Hopkins & Rob Verbeek"
date: "12/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(ggplot2)
library(corrplot)
library(rstanarm)
library("bayesplot")
library(projpred)
library(dplyr)
theme_set(bayesplot::theme_default(base_family = "sans"))
SEED = 1234 
```

## Reading in the ESS Data

```{r selecting data rows and variables}
# N = 1000 # Number of sample points

set.seed(1234)

ESSData <- read.csv(file="data/filtered_data2.csv", header=TRUE, sep=",")

M = length(ESSData$lrscale)
country = ESSData$cntry
age = 2016 - ESSData$yrbrn
age = age/max(age)
drops = c("cntry","yrbrn")
ESSData = ESSData[,!(names(ESSData) %in% drops)] / 10
ESSData = data.frame(age,ESSData)

SAMPLESIZE = 5000

sample_rows = sample(seq(1,M),SAMPLESIZE)

test_rows = sample(seq(1,SAMPLESIZE),1000)

sample_data = data.frame(ESSData[sample_rows,])

training_data = data.frame(sample_data[-test_rows,])

test_data = data.frame(sample_data[test_rows,])
```

```{r correlation matrix}
c = cor(data.frame(sample_data))
lrscale_cor = c["lrscale",]
best_variables = lrscale_cor[order(abs(lrscale_cor),decreasing=TRUE)][1:21]
best_columns = order(abs(lrscale_cor),decreasing=TRUE)[1:21]
sample_data_best_vars = sample_data[best_columns]
training_data_best_vars = training_data[best_columns]
test_data_best_vars = test_data[best_columns]
best_variables
# Look into finding better variables (projpred case studies for variable selection)
```

```{r}
# Try proj pred instead

t_prior <- student_t(df = 5, location = 0.5, scale = 0.1)
fit_lm_all <- stan_glm(lrscale ~ ., data=sample_data, prior = t_prior, prior_intercept = t_prior)
print(fit_lm_all, digits=2)
```

```{r}
# Slow! Run once, save variable
vs <- varsel(fit_lm_all, nv_max=40)
varsel_plot(vs, stats=c('elpd', 'rmse'))
saveRDS(vs, file = "variable_selection_40_tprior.rds")
```

```{r}
# # TOO SLOW
# cvs <- cv_varsel(fit_lm_all)
# suggest_size(cvs)
```
```{r}
varsel_plot(vs, stats = c('elpd', 'rmse'), deltas=T)
```

```{r}
# Try horseshoe prior
t_prior <- student_t(df = 5, location = 0.5, scale = 0.1)
fit_lm_1 <- stan_glm(lrscale ~ gincdif, data=sample_data_best_vars, prior = t_prior, prior_intercept = t_prior)
print(fit_lm_1, digits=2)
```
```{r}
tidy_predictions <- function(mat_pred, df_data, obs_name = "observation",
                             prob_lwr = .025, prob_upr = .975) {
  # Get data-frame with one row per fitted value per posterior sample
  df_pred <- mat_pred %>% 
    as_data_frame %>% 
    setNames(seq_len(ncol(.))) %>% 
    tibble::rownames_to_column("posterior_sample") %>% 
    tidyr::gather_(obs_name, "fitted", setdiff(names(.), "posterior_sample"))
  df_pred
  
  # Helps with joining later
  class(df_pred[[obs_name]]) <- class(df_data[[obs_name]])
  
  # Summarise prediction interval for each observation
  df_pred %>% 
    group_by_(obs_name) %>% 
    summarise(median = median(fitted),
              lower = quantile(fitted, prob_lwr), 
              upper = quantile(fitted, prob_upr)) %>% 
    left_join(df_data, by = obs_name)
}
```

```{r}
g_rng <- range(sample_data$gincdif) 
g_steps <- seq(g_rng[1], g_rng[2], length.out = 80)
new_data <- data.frame(
  observation = seq_along(g_steps), 
  gincdif = g_steps)
pred_post <- posterior_predict(fit_lm_1, newdata = new_data)
dim(pred_post)

df_pred_post <- tidy_predictions(pred_post, new_data)
df_pred_post

```

```{r}
ggplot(sample_n(sample_data_best_vars,400)) + 
  aes(x = jitter(gincdif,1.5)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), data = df_pred_post, 
              alpha = 0.4, fill = "grey60") + 
  geom_line(aes(y = median), data = df_pred_post, colour = "#3366FF", size = 1) + 
  geom_point(aes(y = lrscale))
  # + scale_x_discrete(name ="Government SHould", limits=c(".2","1"), breaks=c("0.2","1"),labels=c("0.2" = "Strongly Agree", "1" = "Strongly Disagree"))
```
At the right end of the plot, some of the posterior predictions at the 95% 
```{r}
last_plot() + 
  geom_hline(yintercept = 1, color = "grey50") + 
  geom_label(x = 0, y = log10(24), label = "24 hours")
```

ADD HERE: TWO LINES DEPENDING ON COUNTRY
```{r, fig.height=5}
# ESSData$cntry
# colors <- ifelse(small_sample$country=="NL", "black", "gray")
sims <- as.matrix(fit_lm_1)
n_sims <- nrow(sims)
subset <- sample(n_sims, 1000)
plot(jitter(small_sample$gincdif,1), jitter(small_sample$lrscale,1), xlab="Government should reduce income differences", ylab="Political Alignment")
for(i in subset){
  abline(sims[i,1], sims[i,2], col="gray")
}
abline(coef(fit_lm_1))
```


```{r, fig.height=5}
ggplot(sample_data, aes(jitter(gincdif,1), jitter(lrscale,1))) +
  geom_point() +
  geom_abline(intercept = coef(fit_lm_1)[1], slope = coef(fit_lm_1)[2]) +
  labs(x = "Government should reduce income differences", y = "Political Alignment")
```
```{r}
fit_lm_2 <- stan_glm(lrscale ~ gincdif + sbbsntx, data=sample_data_best_vars)
summary(fit_lm_2,digits=2)
```

```{r logistic experiment}
fit_lm_logit <- stan_glm(lrscale ~ gincdif, family = binomial(link = "logit"), data=sample_data_best_vars)
summary(fit_lm_logit,digits=2)
```



```{r}

fit_lm_3 <- stan_glm(lrscale ~ ., data=sample_data_best_vars)
print(fit_lm_3)
summary(fit_lm_3,digits=2)
```
```{r pca on all variables}
sample_data_pca <- prcomp(sample_data[,], center = TRUE,scale. = TRUE)
summary(sample_data_pca)
```

```{r}
# posterior_predict (rstanarm)
accuracy <- function(test_data, w) {
  correct = 0
  wrong = 0
  for(n in seq(1:length(test_data[,1]))) {
    cols = seq(2,length(w))
    prediction[n] = (w[cols] %*% as.numeric(test_data[n,cols]) + w[1])
  }

  for(n in seq(1,length(test_data[,1]))) {
      if((prediction[n]<0.5 && test_data$lrscale[n]<0.5) || (prediction[n]>0.5 && test_data$lrscale[n]>0.5)) {
        correct = correct + 1
    }
    else if((prediction[n]<0.5 && test_data$lrscale[n]>0.5) || (prediction[n]>0.5 && test_data$lrscale[n]<0.5)) {
      wrong = wrong + 1
    }
  }
  print(correct)
  print(wrong)
  return(correct/(wrong+correct))
}

print(accuracy(test_data_best_vars[,1:2], fit_lm_1$coefficients))
print(accuracy(test_data_best_vars[,1:3], fit_lm_2$coefficients))
print(accuracy(test_data_best_vars, fit_lm_3$coefficients))
test_data_best_vars
fit_lm_1$coefficients
```
```{r loo comparison}
loo_1 <- loo(fit_lm_1)
loo_2 <- loo(fit_lm_2)
loo_3 <- loo(fit_lm_3)
loo_logit <- loo(fit_lm_logit)
loo_compare(loo_1, loo_3)
loo_1
```