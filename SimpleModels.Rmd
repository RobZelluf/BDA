---
title: "Simple Models"
author: "Daniel Hopkins & Rob Verbeek"
date: "12/1/2019"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(ggplot2)
library(corrplot)
library(rstanarm)
library("bayesplot")
library(projpred)
library(dplyr)
library(lme4)
theme_set(bayesplot::theme_default(base_family = "sans"))
SEED = 1234 
options(mc.cores = parallel::detectCores())
```
## 0. Introduction
We explore what variables predict political affiliation in European countries, and what models allow us to best make those predictions. We take a large dataset from the European Social Survey, clean it, and select the best predictive variables. Then we fit different models to that data. Finally, we fit a hierarchical (partially pooled) model to the data, so that we can preserve the distinctions between different countries, while also taking advantage of common political sentiments across Europe.

## 1. Data
Our data is from the European Social Survey conducted in 2016-2017. The data has more than 30,000 survey results across more than 500 variables in 21 countries. We first filtered the columns with non-spectrum (i.e. categorical) data and also filtered out columns that were country-specific or had a high number of non-answers. After filterning, we were left with nearly 22,000 observations across 105 variables.

```{r selecting data rows and variables}
# N = 1000 # Number of sample points

set.seed(1234)

ESSData <- read.csv(file="data/filtered_data2.csv", header=TRUE, sep=",")

M = length(ESSData$lrscale)
country = ESSData$cntry
age = 2016 - ESSData$yrbrn
age = age/max(age)
drops = c("cntry","yrbrn")
ESSData = ESSData[,!(names(ESSData) %in% drops)] / 10
ESSData = data.frame(age,ESSData)

SAMPLESIZE = 5000

sample_rows = sample(seq(1,M),SAMPLESIZE)

test_rows = sample(seq(1,SAMPLESIZE),1000)

sample_data = data.frame(ESSData[sample_rows,])

training_data = data.frame(sample_data[-test_rows,])

test_data = data.frame(sample_data[test_rows,])
```

## 2. Variable Selection
We used two methods for variable selection. The first method is a simple correlation. Figure 1 plots a correlation matrix for the 20 variables that correlate best with the "lrscale". The plot shows that all of these variables have slight positive or negative correlations with the outcome variable. This is decent selection method. \textit{However}, it fails to account for variables that are highly correlated with eachother. For example, we can see that the variables "imdfetn" ("Allow many/few immigrants of different race/ethnic group from majority") and "impcntr" ("Allow many/few immigrants from poorer countries outside Europe") are highly correlated, so they might carry redundant information for the model. That is why the next method, variable selection using a fitted linear model, is better.
```{r correlation matrix}
c = cor(data.frame(sample_data))
lrscale_corr = c["lrscale",]
corr_best_variables = lrscale_corr[order(abs(lrscale_corr),decreasing=TRUE)][1:21]
corr_best_columns = order(abs(lrscale_corr),decreasing=TRUE)[1:21]
corr_best_variables = colnames(ESSData)[corr_best_columns[2:21]]
corrplot(cor(data.frame(sample_data[corr_best_columns])))
```
Instead of using the correlation matrix method for variable selection, we can fit a generalized linear model where the outcome follows all the 105 variables (excluding the country variable).
```{r cache=TRUE, results=FALSE}
# Try proj pred instead
t_prior <- student_t(df = 5, location = 0.5, scale = 0.1)
fit_lm_all <- stan_glm(lrscale ~ ., data=sample_data, prior = t_prior, prior_intercept = t_prior)
print(fit_lm_all, digits=2)
```

After fitting the model, we use the CRAN varsel() function, which uses cross-validation to pick the variables that contribute most to the model accuracy.
```{r eval = FALSE}
# Slow! Run once, save variable
vs <- varsel(fit_lm_all, nv_max=40)
saveRDS(vs, file = "variable_selection_40_tprior.rds")
```
The five most explanatory variables, according to the varSel function are:

1) gincdif: "Government should reduce differences in income levels (Agree<-->Disagree)"
2) dfincac: "Large differences in income acceptable to reward talents and efforts (Agree<-->Disagree)"
3) impcntr: "Allow many/few immigrants from poorer countries outside Europe (Many<-->Few)"
4) rlgblg: "Belonging to a particular religion or denomination (Yes/No)"
5) sbsrnen: "Favour subsidize renewable energy to reduce climate change (Agree<-->Disagree)"

They are all scale questions except for the binary religion variable.

You can also see the plot, which shows the improved explanatory power of each additional variable. In this case, since the data is complex, the added power is not very great, but the model shows clear improvement across the first five variables, which slows after the fifth variable.

```{r eval=FALSE}
vs = readRDS(file = "variable_selection_40_tprior.rds")
varsel_plot(vs, stats=c('elpd', 'rmse'))
varsel_best_variables = colnames(ESSData)[vs$vind[1:5]]
varsel_best_variables
sample_data_best_5 = cbind(sample_data$lrscale,sample_data[,vs$vind[1:5]])
```

```{r echo=FALSE}
# Hardcoded best variables
best_5 = c(29,22,41,79,85)
sample_data_best_5 = cbind(sample_data$lrscale,sample_data[,best_5])
colnames(sample_data_best_5)[1] = "lrscale"
```

The following plot shows the 95% distribution of posterior draws for the linear weights for each of these five variables and the bias term. Each distribution sits clearly on the positive or negative side of 0, indicating confident predictive value.

```{r five variable linear model, cache=TRUE}
fit_lm_5 = stan_glm(lrscale ~ ., data=sample_data_best_5, prior = t_prior, prior_intercept = t_prior)
```

```{r variable correlation plot}
pplot<-plot(fit_lm_5, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
```


## 3. Graphing and Prediction with Linear Model with One Predictor

Now that we have selected the best variable to use for our models, we can fit a generalized linear model on the first predictor, "Government should reduce differences in income levels" and compare that with the five-variable and all-variable simple linear models.

```{r cache=TRUE, results=FALSE} 
# Try horseshoe prior
t_prior <- student_t(df = 5, location = 0.5, scale = 0.1)
fit_lm_1 <- stan_glm(lrscale ~ gincdif, data=sample_data_best_5, prior = t_prior, prior_intercept = t_prior)
```
```{r}
summary(fit_lm_5)
```

```{r}
summary(fit_lm_1, digits=2)
```


```{r}
tidy_predictions <- function(mat_pred, df_data, obs_name = "observation",
                             prob_lwr = .025, prob_upr = .975) {
  # Get data-frame with one row per fitted value per posterior sample
  df_pred <- mat_pred %>% 
    as_data_frame %>% 
    setNames(seq_len(ncol(.))) %>% 
    tibble::rownames_to_column("posterior_sample") %>% 
    tidyr::gather_(obs_name, "fitted", setdiff(names(.), "posterior_sample"))
  df_pred
  
  # Helps with joining later
  class(df_pred[[obs_name]]) <- class(df_data[[obs_name]])
  
  # Summarise prediction interval for each observation
  df_pred %>% 
    group_by_(obs_name) %>% 
    summarise(median = median(fitted),
              lower = quantile(fitted, prob_lwr), 
              upper = quantile(fitted, prob_upr)) %>% 
    left_join(df_data, by = obs_name)
}
```

```{r}
g_rng <- range(sample_data_best_5$gincdif) 
g_steps <- seq(g_rng[1], g_rng[2], length.out = 80)
new_data <- data.frame(
  observation = seq_along(g_steps), 
  gincdif = g_steps)
pred_post <- posterior_predict(fit_lm_1, newdata = new_data)
dim(pred_post)

df_pred_post <- tidy_predictions(pred_post, new_data)
df_pred_post

```

```{r}
ggplot(sample_n(sample_data_best_5,300)) + 
  aes(x = jitter(gincdif,1.5)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), data = df_pred_post, 
              alpha = 0.4, fill = "grey60") + 
  geom_line(aes(y = median), data = df_pred_post, colour = "#3366FF", size = 1) + 
  geom_point(aes(y = lrscale))
# + scale_x_discrete(name ="Government SHould", limits=c(".2","1"), breaks=c("0.2","1"),labels=c("0.2" = "Strongly Agree", "1" = "Strongly Disagree"))
```

At the right end of the plot, some of the posterior predictions at the 95% level exceed 1, which is a small error with the model.
```{r}
last_plot() + 
  geom_hline(yintercept = 1, color = "grey50") + 
  geom_label(x = 0, y = log10(24), label = "24 hours")
```

```{r}
# sample_data_best_5 = cbind(country[sample_rows], sample_data_best_5)
# colnames(sample_data_best_5)[1] = "country"
# 
# M1 <- lmer(formula = lrscale ~ 1 + gincdif + (1 | country), 
#            data = sample_data_best_5)
# 
# # Complete-pooling regression
# 
# pooled <- lm(formula = lrscale ~ gincdif,
#              data = sample_data_best_5)
# a_pooled <- coef(pooled)[1]   # complete-pooling intercept
# b_pooled <- coef(pooled)[2]   # complete-pooling slope
# 
# # No-pooling regression
# J = 21 # Countries
# nopooled <- lm(formula = lrscale ~ 0 + country + gincdif,
#                data = sample_data_best_5)
# a_nopooled <- coef(nopooled)[1:J]   # 73 no-pooling intercepts              
# b_nopooled <- coef(nopooled)[J+1]
# 
# # Partial pooling (multilevel) regression
# a_part_pooled <- coef(M2)$country[, 1]
# b_part_pooled <- coef(M2)$country[, 2]
```

ADD HERE: TWO LINES DEPENDING ON COUNTRY
```{r, fig.height=5}
# # ESSData$cntry
# # colors <- ifelse(small_sample$country=="NL", "black", "gray")
# sims <- as.matrix(fit_lm_1)
# n_sims <- nrow(sims)
# subset <- sample(n_sims, 1000)
# plot(jitter(small_sample$gincdif,1), jitter(small_sample$lrscale,1), xlab="Government should reduce income differences", ylab="Political Alignment")
# for(i in subset){
#   abline(sims[i,1], sims[i,2], col="gray")
# }
# abline(coef(fit_lm_1))
```


```{r, fig.height=5}
ggplot(sample_data_best_5, aes(jitter(gincdif,1), jitter(lrscale,1))) +
  geom_point() +
  geom_abline(intercept = coef(fit_lm_1)[1], slope = coef(fit_lm_1)[2]) +
  labs(x = "Government should reduce income differences", y = "Political Alignment")
```

```{r}
# fit_lm_2 <- stan_glm(lrscale ~ gincdif + sbbsntx, data=sample_data_best_vars)
# summary(fit_lm_2,digits=2)
```


```{r loo comparison, cache=TRUE}
loo_1 <- loo(fit_lm_1)
loo_5 <- loo(fit_lm_5)
loo_all <- loo(fit_lm_all)
loo_compare(loo_1, loo_5)
loo_compare(loo_5, loo_all)
loo_5
```










UNUSED CODE


```{r}
# 
# fit_lm_3 <- stan_glm(lrscale ~ ., data=sample_data_best_vars)
# print(fit_lm_3)
# summary(fit_lm_3,digits=2)
# ```
# ```{r pca on all variables}
# sample_data_pca <- prcomp(sample_data[,], center = TRUE,scale. = TRUE)
# summary(sample_data_pca)
```

```{r}
# posterior_predict (rstanarm)
# accuracy <- function(test_data, w) {
#   correct = 0
#   wrong = 0
#   for(n in seq(1:length(test_data[,1]))) {
#     cols = seq(2,length(w))
#     prediction[n] = (w[cols] %*% as.numeric(test_data[n,cols]) + w[1])
#   }
#   
#   for(n in seq(1,length(test_data[,1]))) {
#     if((prediction[n]<0.5 && test_data$lrscale[n]<0.5) || (prediction[n]>0.5 && test_data$lrscale[n]>0.5)) {
#       correct = correct + 1
#     }
#     else if((prediction[n]<0.5 && test_data$lrscale[n]>0.5) || (prediction[n]>0.5 && test_data$lrscale[n]<0.5)) {
#       wrong = wrong + 1
#     }
#   }
#   print(correct)
#   print(wrong)
#   return(correct/(wrong+correct))
# }
# 
# print(accuracy(test_data_best_vars[,1:2], fit_lm_1$coefficients))
# print(accuracy(test_data_best_vars[,1:3], fit_lm_2$coefficients))
# print(accuracy(test_data_best_vars, fit_lm_3$coefficients))
# test_data_best_vars
# fit_lm_1$coefficients
```