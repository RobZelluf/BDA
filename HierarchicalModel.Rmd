---
title: "Simple Models"
author: "Daniel Hopkins & Rob Verbeek"
date: "8/12/2019"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(ggplot2)
library(corrplot)
library(rstanarm)
library("bayesplot")
library(projpred)
library(dplyr)
library(lme4)
theme_set(bayesplot::theme_default(base_family = "sans"))
SEED = 1234 
options(mc.cores = parallel::detectCores())

```
## 0. Introduction
We explore what variables predict political affiliation in European countries, and what models allow us to best make those predictions. We take a large dataset from the European Social Survey, clean it, and select the best predictive variables. Then we fit different models to that data. Finally, we fit a hierarchical (partially pooled) model to the data, so that we can preserve the distinctions between different countries, while also taking advantage of common political sentiments across Europe.

## 1. Data
Our data is from the European Social Survey conducted in 2016-2017. The data has more than 30,000 survey results across more than 500 variables in 21 countries. We first filtered the columns with non-spectrum (i.e. categorical) data and also filtered out columns that were country-specific or had a high number of non-answers. After filterning, we were left with nearly 22,000 observations across 105 variables.

### Data filtering
The ESS dataset is very large, with a lot of different types of values and country-specific questions. So before running our models, we cleaned our data. Cleaning the data consists of the following steps:

Remove country-specific data:
- We want all variables to be available for all countries, so we remove country-specific data.

Remove nominal data:
- The dataset has a lot of questions that are nominal (things like job-type, political party voted for). These are filtered out, because they cannot be put on a scale.

Remove sparse questions:
- We remove the questions that have more tan 10% invalid answers (e.g. not answered, no preference, etc..). This is mainly done so that in a later stage of filtering, these questions don't cause the filtering of too many datapoints.

Change scales:
- The questions come in a lot of different scales, like 1-10, 1-5, 1-4 but also yes-no questions. These questions are all scaled to the 1-1- scale. For most scales this is done using a simple transformation, the yes-no questions are mapped to the values 7 and 3.

```{r selecting data rows and variables}
ESSData <- read.csv(file="data/filtered_data2.csv", header=TRUE, sep=",")

M = length(ESSData$lrscale)
country = ESSData$cntry
age = 2016 - ESSData$yrbrn
age = age/max(age)
drops = c("cntry","yrbrn")
ESSData = ESSData[,!(names(ESSData) %in% drops)] / 10
ESSData = data.frame(age,ESSData)

SAMPLESIZE = 5000

sample_rows = sample(seq(1,M),SAMPLESIZE)
test_rows = sample(seq(1,SAMPLESIZE),1000)

sample_data = data.frame(ESSData[sample_rows,])
training_data = data.frame(sample_data[-test_rows,])
test_data = data.frame(sample_data[test_rows,])
```

## 2. Variable Selection
We used two methods for variable selection. The first method is a simple correlation. Figure 1 plots a correlation matrix for the 20 variables that correlate best with the "lrscale". The plot shows that all of these variables have slight positive or negative correlations with the outcome variable. This is decent selection method. \textit{However}, it fails to account for variables that are highly correlated with eachother. For example, we can see that the variables "imdfetn" ("Allow many/few immigrants of different race/ethnic group from majority") and "impcntr" ("Allow many/few immigrants from poorer countries outside Europe") are highly correlated, so they might carry redundant information for the model. That is why the next method, variable selection using a fitted linear model, is better.

```{r correlation matrix}
c = cor(data.frame(sample_data))
lrscale_corr = c["lrscale",]
corr_best_variables = lrscale_corr[order(abs(lrscale_corr),decreasing=TRUE)][1:21]
corr_best_columns = order(abs(lrscale_corr),decreasing=TRUE)[1:21]
corr_best_variables = colnames(ESSData)[corr_best_columns[2:21]]
corrplot(cor(data.frame(sample_data[corr_best_columns])))
```










## Pooled model
The pooled model considers all data the same, thus not take into account the country each datapoint is from. It fits one linear model to predict the left-right scale, based on all input variables. 

## Partially pooled
The previous model is pooled, assuming there is no differences between countries. The partially pooled model allows for differentation between countries. Each country will have it's own intecept and slope per variable, but these parameters are fitted to an overall distribution. This still allows the model to generalize over the entire dataset, while being able to fit a more narrow distribution on each country individually.

```{r cache=TRUE}
# Try horseshoe prior
t_prior <- student_t(df = 5, location = 0.5, scale = 0.1)
fit1 <- stan_glm(lrscale ~ gincdif, data=sample_data_best_5, prior = t_prior, prior_intercept = t_prior)
```

```{r cache=TRUE}
fit_hier_2 <- stan_lmer(lrscale ~ 1 + gincdif + impcntr + dfincac + rlgblg + sbsrnen + (1 + gincdif + impcntr + dfincac + rlgblg + sbsrnen | country), data=sample_data_best_5, iter=200)
print(fit_hier_2, digits=2)
```

```{r}
coefficients <- coef(fit_hier_2)[1]
```
## Interesting countries:
Sweden: Really steep slopes for most variables
Poland and Siberia: slope different sign than other countries for many variables
Italy: Very large slope for impcntr
EE: Max slope for rlgblg
LT: Min slope for impcntr
CZ: Min slope for sbsrnen

```{r}

# (0) Set axes & choose schools
y <- sample_data_best_5$lrscale
x <- sample_data_best_5$gincdif
countryid <- sample_data_best_5$country
sel.cntry <- c("SE",
             "PL",
             "SI",
             "IT",
             "EE",
             "LT",
             "CZ",
             "FI",
             "NL")

# (1) Subset 8 of the schools; generate data frame
df <- data.frame(y, x, countryid)
df8 <- subset(df, countryid %in% sel.cntry)

# (2) Assign complete-pooling, no-pooling, partial pooling estimates
a_pooled <- coef(fit1)[1]
b_pooled <- coef(fit1)[2]

a_part_pooled <- coef(fit_hier_2)$country[, 1]
b_part_pooled <- coef(fit_hier_2)$country[, 2]

df8$a_pooled <- a_pooled 
df8$b_pooled <- b_pooled

df8$a_part_pooled <- a_part_pooled[df8$countryid]
df8$b_part_pooled <- b_part_pooled[df8$countryid]

ggplot(data = df8, 
       aes(x = x, y = y)) + 
  facet_wrap(facets = ~ countryid, 
             ncol = 4) + 
  theme_bw() +
  geom_jitter(position = position_jitter(width = .05, 
                                         height = 0.02)) +
  geom_abline(aes(intercept = a_part_pooled, 
                  slope = b_part_pooled), 
              linetype = "solid", 
              color = "blue", 
              size = 0.5) +
  
  geom_abline(aes(intercept = a_pooled, 
                slope = b_pooled), 
            linetype = "solid", 
            color = "red", 
            size = 0.5) +
  
  scale_x_continuous(breaks = c(0, 1)) + 
  labs(title = "Complete-pooling, No-pooling, and Partial pooling estimates",
       x = "", 
       y = "Total score on coursework paper")+theme_bw( base_family = "serif")

```




```