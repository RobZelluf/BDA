---
title: "Simple Models"
author: "Daniel Hopkins & Rob Verbeek"
date: "8/12/2019"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: 2
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(ggplot2)
library(corrplot)
library(rstanarm)
library("bayesplot")
library(projpred)
library(dplyr)
library(lme4)
theme_set(bayesplot::theme_default(base_family = "sans"))
SEED = 1234 
options(mc.cores = parallel::detectCores())

```
## 0. Introduction
We explore what variables predict political affiliation in European countries, and what models allow us to best make those predictions. We take a large dataset from the European Social Survey, filter and clean it, and select the best predictive variables using two different techniques. Then we fit a linear model to the data, comparing the one-predictor and five-predictor models. Finally, we fit a hierarchical (partially pooled) model to the data, so that we can preserve the distinctions between different countries, while also taking advantage of common political sentiments across Europe. Then we show that this model performs better than the pooled model at prediction and validation.

## 1. Data
Our data is from the European Social Survey conducted in 2016-2017. The data has more than 30,000 survey results across more than 500 variables in 21 countries. We first filtered the columns with non-spectrum (i.e. categorical) data and also filtered out columns that were country-specific or had a high number of non-answers. After filterning, we were left with nearly 22,000 observations across 105 variables.

### Data filtering
The ESS dataset is very large, with a lot of different types of values and country-specific questions. So before running our models, we cleaned our data. Cleaning the data consists of the following steps:

Remove country-specific data:
- We want all variables to be available for all countries, so we remove country-specific data.

Remove nominal data:
- The dataset has a lot of questions that are nominal (things like job-type, political party voted for). These are filtered out, because they cannot be put on a scale.

Remove sparse questions:
- We remove the questions that have more tan 10% invalid answers (e.g. not answered, no preference, etc..). This is mainly done so that in a later stage of filtering, these questions don't cause the filtering of too many datapoints.

Change scales:
- The questions come in a lot of different scales, like 1-10, 1-5, 1-4 but also yes-no questions. These questions are all scaled to the 1-1- scale. For most scales this is done using a simple transformation, the yes-no questions are mapped to the values 7 and 3.

```{r selecting data rows and variables}
ESSData <- read.csv(file="data/filtered_data2.csv", header=TRUE, sep=",")

M = length(ESSData$lrscale)
country = ESSData$cntry
age = 2016 - ESSData$yrbrn
age = age/max(age)
drops = c("cntry","yrbrn")
ESSData = ESSData[,!(names(ESSData) %in% drops)] / 10
ESSData = data.frame(age,ESSData)

SAMPLESIZE = 5000

sample_rows = sample(seq(1,M),SAMPLESIZE)
test_rows = sample(seq(1,SAMPLESIZE),1000)

sample_data = data.frame(ESSData[sample_rows,])
training_data = data.frame(sample_data[-test_rows,])
test_data = data.frame(sample_data[test_rows,])
```

## 2. Variable Selection
We used two methods for variable selection. The first method is a simple correlation. Figure 1 plots a correlation matrix for the 20 variables that correlate best with the "lrscale". The plot shows that all of these variables have slight positive or negative correlations with the outcome variable. This is decent selection method. \textit{However}, it fails to account for variables that are highly correlated with eachother. For example, we can see that the variables "imdfetn" ("Allow many/few immigrants of different race/ethnic group from majority") and "impcntr" ("Allow many/few immigrants from poorer countries outside Europe") are highly correlated, so they might carry redundant information for the model. That is why the next method, variable selection using a fitted linear model and the varsel() function, is better.

```{r correlation matrix}
c = cor(data.frame(sample_data))
lrscale_corr = c["lrscale",]
corr_best_variables = lrscale_corr[order(abs(lrscale_corr),decreasing=TRUE)][1:21]
corr_best_columns = order(abs(lrscale_corr),decreasing=TRUE)[1:21]
corr_best_variables = colnames(ESSData)[corr_best_columns[2:21]]
corrplot(cor(data.frame(sample_data[corr_best_columns])))
```

Instead of using the correlation matrix method for variable selection, we can fit a generalized linear model where the outcome follows all the 105 variables (excluding the country variable).

### Note about Priors

For all models in this report, we use a weakly informative student-t prior. We choose student-t because it has fatter tails than the gaussian distribution, reflecting the greater frequency of extreme political beliefs.

```{r cache=TRUE, results=FALSE}
t_prior <- student_t(df = 5, location = 0.5, scale = 0.1)
fit_lm_all <- stan_glm(lrscale ~ ., data=sample_data, prior = t_prior, prior_intercept = t_prior)
summary(fit_lm_all, digits=2)
```

After fitting the model, we use the CRAN varsel() function, which uses cross-validation to pick the variables that contribute most to the model accuracy.

```{r eval = FALSE}
# Slow! Run once, save variable
vs <- varsel(fit_lm_all, nv_max=40)
saveRDS(vs, file = "variable_selection_40_tprior.rds")
```
The five most explanatory variables, according to the varSel function are:

1) gincdif: "Government should reduce differences in income levels (Agree<-->Disagree)"
2) dfincac: "Large differences in income acceptable to reward talents and efforts (Agree<-->Disagree)"
3) impcntr: "Allow many/few immigrants from poorer countries outside Europe (Many<-->Few)"
4) rlgblg: "Belonging to a particular religion or denomination (Yes/No)"
5) sbsrnen: "Favour subsidize renewable energy to reduce climate change (Agree<-->Disagree)"

They are all scale questions except for the binary religion variable.

You can also see the plot, which shows the improved explanatory power of each additional variable. In this case, since the data is complex, the added power is not very great, but the model shows clear improvement across the first five variables, which slows after the fifth variable.

```{r eval=FALSE}
vs = readRDS(file = "variable_selection_40_tprior.rds")
varsel_plot(vs, stats=c('elpd', 'rmse'))
varsel_best_variables = colnames(ESSData)[vs$vind[1:5]]
varsel_best_variables
sample_data_best_5 = cbind(sample_data$lrscale,sample_data[,vs$vind[1:5]])
```

```{r echo=FALSE}
# Hardcoded best variables
best_5 = c(29,22,41,79,85)
sample_data_best_5 = cbind(sample_data$lrscale,sample_data[,best_5])
colnames(sample_data_best_5)[1] = "lrscale"
```

The following plot shows the 95% distribution of posterior draws for the linear weights for each of these five variables and the bias term. Each distribution sits clearly on the positive or negative side of 0, indicating confident predictive value.

```{r five variable linear model, cache=TRUE}
fit_lm_5 = stan_glm(lrscale ~ ., data=sample_data_best_5, prior = t_prior, prior_intercept = t_prior)
```

```{r variable correlation plot}
pplot<-plot(fit_lm_5, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
```

## 3. Graphing and Prediction with Linear Model with One Predictor

Now that we have selected the best variable to use for our models, we can fit a generalized linear model on the first predictor, "Government should reduce differences in income levels" and compare that with the five-variable and all-variable simple linear models.

```{r cache=TRUE, results=FALSE} 
# Try horseshoe prior
t_prior <- student_t(df = 5, location = 0.5, scale = 0.1)
fit_lm_1 <- stan_glm(lrscale ~ gincdif, data=sample_data_best_5, prior = t_prior, prior_intercept = t_prior)
```


```{r}
summary(fit_lm_1, digits=2)
```

Best fit line
```{r, fig.height=5}
ggplot(sample_data_best_5, aes(jitter(gincdif,1), jitter(lrscale,1))) +
  geom_point() +
  geom_abline(intercept = coef(fit_lm_1)[1], slope = coef(fit_lm_1)[2]) +
  labs(x = "Government should reduce income differences", y = "Political Alignment")
```

```{r echo=FALSE}
# Credit for this plot: 
tidy_predictions <- function(mat_pred, df_data, obs_name = "observation",
                             prob_lwr = .025, prob_upr = .975) {
  # Get data-frame with one row per fitted value per posterior sample
  df_pred <- mat_pred %>% 
    as_data_frame %>% 
    setNames(seq_len(ncol(.))) %>% 
    tibble::rownames_to_column("posterior_sample") %>% 
    tidyr::gather_(obs_name, "fitted", setdiff(names(.), "posterior_sample"))
  df_pred
  
  # Helps with joining later
  class(df_pred[[obs_name]]) <- class(df_data[[obs_name]])
  
  # Summarise prediction interval for each observation
  df_pred %>% 
    group_by_(obs_name) %>% 
    summarise(median = median(fitted),
              lower = quantile(fitted, prob_lwr), 
              upper = quantile(fitted, prob_upr)) %>% 
    left_join(df_data, by = obs_name)
}
```

```{r}
g_rng <- range(sample_data_best_5$gincdif) 
g_steps <- seq(g_rng[1], g_rng[2], length.out = 80)
new_data <- data.frame(
  observation = seq_along(g_steps), 
  gincdif = g_steps)
pred_post <- posterior_predict(fit_lm_1, newdata = new_data)
dim(pred_post)

df_pred_post <- tidy_predictions(pred_post, new_data)
df_pred_post

```

```{r}
ggplot(sample_n(sample_data_best_5,300)) + 
  aes(x = jitter(gincdif,1.5)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), data = df_pred_post, 
              alpha = 0.4, fill = "grey60") + 
  geom_line(aes(y = median), data = df_pred_post, colour = "#3366FF", size = 1) + 
  geom_point(aes(y = lrscale))
# + scale_x_discrete(name ="Government SHould", limits=c(".2","1"), breaks=c("0.2","1"),labels=c("0.2" = "Strongly Agree", "1" = "Strongly Disagree"))
```

At the right end of the plot, some of the posterior predictions at the 95% level exceed 1, which is a small error with the model.
```{r}
last_plot() + 
  geom_hline(yintercept = 1, color = "grey50") + 
  geom_label(x = 0, y = log10(24), label = "24 hours")
```





## Pooled model
The pooled model considers all data the same, thus not take into account the country each datapoint is from. It fits one linear model to predict the left-right scale, based on all input variables. 

Below is a summary of the simple linear five-predictor pooled model. Note that all rhat values are 1.0, indicating good convergence and independence of of the MCMC

```{r}
summary(fit_lm_5)
```

## Partially pooled
The previous model is pooled, assuming there is no differences between countries. The partially pooled model allows for differentation between countries. Each country will have it's own intecept and slope per variable, but these parameters are fitted to an overall distribution. This still allows the model to generalize over the entire dataset, while being able to fit a more narrow distribution on each country individually.


## Separate model
```{r cache=TRUE}
nopooled <- stan_lmer(formula = lrscale ~ 0 + (1 + gincdif + impcntr + dfincac + rlgblg + sbsrnen | country),
               data = sample_data_best_5, iter=200)

coef(nopooled)
```

## Hierarchical model
```{r cache=TRUE}
fit_hier_2 <- stan_lmer(lrscale ~ 1 + gincdif + impcntr + dfincac + rlgblg + sbsrnen + (1 + gincdif + impcntr + dfincac + rlgblg + sbsrnen | country), data=sample_data_best_5, iter=200)
print(fit_hier_2, digits=2)
```

```{r}
coefficients <- coef(fit_hier_2)[1]
```
## Interesting countries:
Sweden: Really steep slopes for most variables
Poland and Siberia: slope different sign than other countries for many variables
Italy: Very large slope for impcntr
EE: Max slope for rlgblg
LT: Min slope for impcntr
CZ: Min slope for sbsrnen

```{r}

# (0) Set axes & choose schools
y <- sample_data_best_5$lrscale
x <- sample_data_best_5$gincdif
countryid <- sample_data_best_5$country
sel.cntry <- c("SE",
             "PL",
             "SI",
             "IT",
             "EE",
             "LT",
             "CZ",
             "FI",
             "NL")

# (1) Subset 8 of the schools; generate data frame
df <- data.frame(y, x, countryid)
df8 <- subset(df, countryid %in% sel.cntry)

# (2) Assign complete-pooling, no-pooling, partial pooling estimates
a_pooled <- coef(fit_lm_1)[1]
b_pooled <- coef(fit_lm_1)[2]

a_part_pooled <- coef(fit_hier_2)$country[, 1]
b_part_pooled <- coef(fit_hier_2)$country[, 2]

df8$a_pooled <- a_pooled 
df8$b_pooled <- b_pooled

df8$a_part_pooled <- a_part_pooled[df8$countryid]
df8$b_part_pooled <- b_part_pooled[df8$countryid]

ggplot(data = df8, 
       aes(x = x, y = y)) + 
  facet_wrap(facets = ~ countryid, 
             ncol = 4) + 
  theme_bw() +
  geom_jitter(position = position_jitter(width = .05, 
                                         height = 0.02)) +
  geom_abline(aes(intercept = a_part_pooled, 
                  slope = b_part_pooled), 
              linetype = "solid", 
              color = "blue", 
              size = 0.5) +
  
  geom_abline(aes(intercept = a_pooled, 
                slope = b_pooled), 
            linetype = "solid", 
            color = "red", 
            size = 0.5) +
  
  scale_x_continuous(breaks = c(0, 1)) + 
  labs(title = "Complete-pooling, No-pooling, and Partial pooling estimates",
       x = "", 
       y = "Total score on coursework paper")+theme_bw( base_family = "serif")

```


## Validation and Leave-One-Out Analysis

Below, we can see the loo output for the best 5 variable simple linear model. All k values are less than 0.5, so the loo output has good convergence. The p_loo value is 7.7, which is quite high for a ten parameter model (11 parameters including the bias term).

```{r loo comparison, cache=TRUE}
loo_1 <- loo(fit_lm_1)
loo_5 <- loo(fit_lm_5)
loo_all <- loo(fit_lm_all)
loo_hier <- loo(fit_hier_2)
loo_5
```
Loo_compare() also shows a clear preference for the five-variable model over the one-variable model. The elpd difference is 174.2 between the two models, with a standard error of only 19.9, so the preference is highly significant.
```{r}
loo_compare(loo_5, loo_1)
```


```{r}
loo_compare(loo_hier, loo_5)
```

```{r loo output}
loo_5
```

## Conclusion

The main conclusion is that, while Europe has common trends in what it means to identify as "left" and "right" on the political spectrum, but major differences still exist between different countries. These terms don't mean the same thing in each country, as you can see in the partially-pooled plots comparing, for example, Sweden and Poland.

Despite the differences, some trends are as-expected. For example, friendliness to immigrants certainly correlates, in most countries, with being more "left" on the political spectrum. In the countries that see different correlations, perhaps it is because--in the european context--they see themselves as possible migrants to other, wealthier European countries. 

We are especially interested in those countries where the first variable "Government should reduce differences in income levels" does not correlate with the traditional left-right spectrum. We discussed with a Polish friend and learned that "left" in Poland is seen differently from the way it is seen in Western Europe, perhaps due to historical issues with communism and the USSR. We have no basis to make further claims, but it is an interesting avenue for research.


